---
language:
- en
- ko
license: llama3
library_name: transformers
base_model:
- meta-llama/Meta-Llama-3-8B
---

<a href="https://github.com/MLP-Lab/Bllossom">
  <img src="https://github.com/teddysum/bllossom/blob/main//bllossom_icon.png?raw=true" width="40%" height="50%">
</a>



# Update!
* [2024.06.18] ì‚¬ì „í•™ìŠµëŸ‰ì„ **250GB**ê¹Œì§€ ëŠ˜ë¦° Bllossom ELOëª¨ë¸ë¡œ ì—…ë°ì´íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ë‹¨ì–´í™•ì¥ì€ í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ ë‹¨ì–´í™•ì¥ëœ long-context ëª¨ë¸ì„ í™œìš©í•˜ê³  ì‹¶ìœ¼ì‹ ë¶„ì€ ê°œì¸ì—°ë½ì£¼ì„¸ìš”!
* [2024.06.18] Bllossom ELO ëª¨ë¸ì€ ìì²´ ê°œë°œí•œ ELOì‚¬ì „í•™ìŠµ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ í•™ìŠµëœ ëª¨ë¸ì…ë‹ˆë‹¤. [LogicKor](https://github.com/StableFluffy/LogicKor) ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ í˜„ì¡´í•˜ëŠ” í•œêµ­ì–´ 10Bì´í•˜ ëª¨ë¸ì¤‘ SOTAì ìˆ˜ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤. 

LogicKor ì„±ëŠ¥í‘œ :
| Model | Math | Reasoning | Writing | Coding | Understanding | Grammar | Single ALL | Multi ALL | Overall |
|:---------:|:-----:|:------:|:-----:|:-----:|:----:|:-----:|:-----:|:-----:|:----:|
| gpt-3.5-turbo-0125 | 7.14 | 7.71 | 8.28 | 5.85 | 9.71 | 6.28 | 7.50 | 7.95 | 7.72 |
| gemini-1.5-pro-preview-0215 | 8.00 | 7.85 | 8.14 | 7.71 | 8.42 | 7.28 | 7.90 | 6.26 | 7.08 |
| llama-3-Korean-Bllossom-8B | 5.43 | 8.29 | 9.0 | 4.43 | 7.57 | 6.86 | 6.93 | 6.93 | 6.93 |



# Bllossom | [Demo]() | [Homepage](https://www.bllossom.ai/) | [Github](https://github.com/MLP-Lab/Bllossom) |

<!-- [GPUìš© Colab ì½”ë“œì˜ˆì œ](https://colab.research.google.com/drive/1fBOzUVZ6NRKk_ugeoTbAOokWKqSN47IG?usp=sharing) | -->
<!-- [CPUìš© Colab ì–‘ìí™”ëª¨ë¸ ì½”ë“œì˜ˆì œ](https://colab.research.google.com/drive/129ZNVg5R2NPghUEFHKF0BRdxsZxinQcJ?usp=drive_link) -->

```bash
ì €í¬ BllossomíŒ€ ì—ì„œ í•œêµ­ì–´-ì˜ì–´ ì´ì¤‘ ì–¸ì–´ëª¨ë¸ì¸ Bllossomì„ ê³µê°œí–ˆìŠµë‹ˆë‹¤!
ì„œìš¸ê³¼ê¸°ëŒ€ ìŠˆí¼ì»´í“¨íŒ… ì„¼í„°ì˜ ì§€ì›ìœ¼ë¡œ 100GBê°€ë„˜ëŠ” í•œêµ­ì–´ë¡œ ëª¨ë¸ì „ì²´ë¥¼ í’€íŠœë‹í•œ í•œêµ­ì–´ ê°•í™” ì´ì¤‘ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤!
í•œêµ­ì–´ ì˜í•˜ëŠ” ëª¨ë¸ ì°¾ê³  ìˆì§€ ì•Šìœ¼ì…¨ë‚˜ìš”?
 - í•œêµ­ì–´ ìµœì´ˆ! ë¬´ë ¤ 3ë§Œê°œê°€ ë„˜ëŠ” í•œêµ­ì–´ ì–´íœ˜í™•ì¥
 - Llama3ëŒ€ë¹„ ëŒ€ëµ 25% ë” ê¸´ ê¸¸ì´ì˜ í•œêµ­ì–´ Context ì²˜ë¦¬ê°€ëŠ¥
 - í•œêµ­ì–´-ì˜ì–´ Pararell Corpusë¥¼ í™œìš©í•œ í•œêµ­ì–´-ì˜ì–´ ì§€ì‹ì—°ê²° (ì‚¬ì „í•™ìŠµ)
 - í•œêµ­ì–´ ë¬¸í™”, ì–¸ì–´ë¥¼ ê³ ë ¤í•´ ì–¸ì–´í•™ìê°€ ì œì‘í•œ ë°ì´í„°ë¥¼ í™œìš©í•œ ë¯¸ì„¸ì¡°ì •
 - ê°•í™”í•™ìŠµ
ì´ ëª¨ë“ ê²Œ í•œêº¼ë²ˆì— ì ìš©ë˜ê³  ìƒì—…ì  ì´ìš©ì´ ê°€ëŠ¥í•œ Bllossomì„ ì´ìš©í•´ ì—¬ëŸ¬ë¶„ ë§Œì˜ ëª¨ë¸ì„ ë§Œë“¤ì–´ë³´ì„¸ìš¥!
ë¬´ë ¤ Colab ë¬´ë£Œ GPUë¡œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. í˜¹ì€ ì–‘ìí™” ëª¨ë¸ë¡œ CPUì—ì˜¬ë ¤ë³´ì„¸ìš” [ì–‘ìí™”ëª¨ë¸](https://huggingface.co/MLP-KTLim/llama-3-Korean-Bllossom-8B-4bit)

1. Bllossom-8BëŠ” ì„œìš¸ê³¼ê¸°ëŒ€, í…Œë””ì¸, ì—°ì„¸ëŒ€ ì–¸ì–´ìì› ì—°êµ¬ì‹¤ì˜ ì–¸ì–´í•™ìì™€ í˜‘ì—…í•´ ë§Œë“  ì‹¤ìš©ì£¼ì˜ê¸°ë°˜ ì–¸ì–´ëª¨ë¸ì…ë‹ˆë‹¤! ì•ìœ¼ë¡œ ì§€ì†ì ì¸ ì—…ë°ì´íŠ¸ë¥¼ í†µí•´ ê´€ë¦¬í•˜ê² ìŠµë‹ˆë‹¤ ë§ì´ í™œìš©í•´ì£¼ì„¸ìš” ğŸ™‚
2. ì´ˆ ê°•ë ¥í•œ Advanced-Bllossom 8B, 70Bëª¨ë¸, ì‹œê°-ì–¸ì–´ëª¨ë¸ì„ ë³´ìœ í•˜ê³  ìˆìŠµë‹ˆë‹¤! (ê¶ê¸ˆí•˜ì‹ ë¶„ì€ ê°œë³„ ì—°ë½ì£¼ì„¸ìš”!!)
3. Bllossomì€ NAACL2024, LREC-COLING2024 (êµ¬ë‘) ë°œí‘œë¡œ ì±„íƒë˜ì—ˆìŠµë‹ˆë‹¤.
4. ì¢‹ì€ ì–¸ì–´ëª¨ë¸ ê³„ì† ì—…ë°ì´íŠ¸ í•˜ê² ìŠµë‹ˆë‹¤!! í•œêµ­ì–´ ê°•í™”ë¥¼ìœ„í•´ ê³µë™ ì—°êµ¬í•˜ì‹¤ë¶„(íŠ¹íˆë…¼ë¬¸) ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤!! 
   íŠ¹íˆ ì†ŒëŸ‰ì˜ GPUë¼ë„ ëŒ€ì—¬ ê°€ëŠ¥í•œíŒ€ì€ ì–¸ì œë“  ì—°ë½ì£¼ì„¸ìš”! ë§Œë“¤ê³  ì‹¶ì€ê±° ë„ì™€ë“œë ¤ìš”.
```

The Bllossom language model is a Korean-English bilingual language model based on the open-source LLama3. It enhances the connection of knowledge between Korean and English. It has the following features:

* **Knowledge Linking**: Linking Korean and English knowledge through additional training
* **Vocabulary Expansion**: Expansion of Korean vocabulary to enhance Korean expressiveness.
* **Instruction Tuning**: Tuning using custom-made instruction following data specialized for Korean language and Korean culture
* **Human Feedback**: DPO has been applied
* **Vision-Language Alignment**: Aligning the vision transformer with this language model 

**This model developed by [MLPLab at Seoultech](http://mlp.seoultech.ac.kr), [Teddysum](http://teddysum.ai/) and [Yonsei Univ](https://sites.google.com/view/hansaemkim/hansaem-kim)**

## Demo Video

<div style="display: flex; justify-content: space-between;">
  <!-- ì²« ë²ˆì§¸ ì»¬ëŸ¼ -->
  <div style="width: 49%;">
    <a>
      <img src="https://github.com/lhsstn/lhsstn/blob/main/x-llava_dem.gif?raw=true" style="width: 100%; height: auto;">
    </a>
    <p style="text-align: center;">Bllossom-V Demo</p>
  </div>

  <!-- ë‘ ë²ˆì§¸ ì»¬ëŸ¼ (í•„ìš”í•˜ë‹¤ë©´) -->
  <div style="width: 49%;">
    <a>
      <img src="https://github.com/lhsstn/lhsstn/blob/main/bllossom_demo_kakao.gif?raw=true" style="width: 70%; height: auto;">
    </a>
    <p style="text-align: center;">Bllossom Demo(Kakao)ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤ã…¤</p>
  </div>
</div>



# NEWS
* [2024.06.18] We have reverted to the non-vocab-expansion model. However, we have significantly increased the amount of pre-training data to 250GB.
* [2024.05.08] Vocab Expansion Model Update
* [2024.04.25] We released Bllossom v2.0, based on llama-3

## Example code

### Colab Tutorial
 - [Inference-Code-Link](https://colab.research.google.com/drive/1fBOzUVZ6NRKk_ugeoTbAOokWKqSN47IG?usp=sharing)

### Install Dependencies
```bash
pip install torch transformers==4.40.0 accelerate
```

### Python code with Pipeline
```python
import transformers
import torch

model_id = "MLP-KTLim/llama-3-Korean-Bllossom-8B"

pipeline = transformers.pipeline(
    "text-generation",
    model=model_id,
    model_kwargs={"torch_dtype": torch.bfloat16},
    device_map="auto",
)

pipeline.model.eval()

PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.'''
instruction = "ì„œìš¸ì˜ ìœ ëª…í•œ ê´€ê´‘ ì½”ìŠ¤ë¥¼ ë§Œë“¤ì–´ì¤„ë˜?"

messages = [
    {"role": "system", "content": f"{PROMPT}"},
    {"role": "user", "content": f"{instruction}"}
    ]

prompt = pipeline.tokenizer.apply_chat_template(
        messages, 
        tokenize=False, 
        add_generation_prompt=True
)

terminators = [
    pipeline.tokenizer.eos_token_id,
    pipeline.tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = pipeline(
    prompt,
    max_new_tokens=2048,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9
)

print(outputs[0]["generated_text"][len(prompt):])

# ë¬¼ë¡ ì´ì£ ! ì„œìš¸ì€ ë‹¤ì–‘í•œ ë¬¸í™”ì™€ ì—­ì‚¬, ìì—°ì„ ê²¸ë¹„í•œ ë„ì‹œë¡œ, ë§ì€ ê´€ê´‘ ëª…ì†Œë¥¼ ìë‘í•©ë‹ˆë‹¤. ì—¬ê¸° ì„œìš¸ì˜ ìœ ëª…í•œ ê´€ê´‘ ì½”ìŠ¤ë¥¼ ì†Œê°œí•´ ë“œë¦´ê²Œìš”.

### ì½”ìŠ¤ 1: ì—­ì‚¬ì™€ ë¬¸í™” íƒë°©

1. **ê²½ë³µê¶**
   - ì„œìš¸ì˜ ëŒ€í‘œì ì¸ ê¶ê¶ë¡œ, ì¡°ì„  ì™•ì¡°ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ë¥¼ ì²´í—˜í•  ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤.

2. **ë¶ì´Œ í•œì˜¥ë§ˆì„**
   - ì „í†µ í•œì˜¥ì´ ì˜ ë³´ì¡´ëœ ë§ˆì„ë¡œ, ì¡°ì„ ì‹œëŒ€ì˜ ìƒí™œìƒì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **ì¸ì‚¬ë™**
   - ì „í†µ ë¬¸í™”ì™€ í˜„ëŒ€ ì˜ˆìˆ ì´ ê³µì¡´í•˜ëŠ” ê±°ë¦¬ë¡œ, ë‹¤ì–‘í•œ ê°¤ëŸ¬ë¦¬ì™€ ì „í†µ ìŒì‹ì ì´ ìˆìŠµë‹ˆë‹¤.

4. **ì²­ê³„ì²œ**
   - ì„œìš¸ì˜ ì¤‘ì‹¬ì— ìœ„ì¹˜í•œ ì²œë¬¸ìœ¼ë¡œ, ì¡°ê¹…ê³¼ ì‚°ì±…ì„ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤.

### ì½”ìŠ¤ 2: ìì—°ê³¼ ì‡¼í•‘

1. **ë‚¨ì‚° ì„œìš¸íƒ€ì›Œ**
   - ì„œìš¸ì˜ ì „ê²½ì„ í•œëˆˆì— ë³¼ ìˆ˜ ìˆëŠ” ê³³ìœ¼ë¡œ, íŠ¹íˆ ì €ë… ì‹œê°„ëŒ€ì— ì¼ëª°ì„ ê°ìƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

2. **ëª…ë™**
   - ì‡¼í•‘ê³¼ ìŒì‹ì ì´ ì¦ë¹„í•œ ì§€ì—­ìœ¼ë¡œ, ë‹¤ì–‘í•œ ë¸Œëœë“œì™€ ì „í†µ ìŒì‹ì„ ë§›ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **í•œê°•ê³µì›**
   - ì„œìš¸ì˜ ì£¼ìš” ê³µì› ì¤‘ í•˜ë‚˜ë¡œ, ì¡°ê¹…, ìì „ê±° íƒ€ê¸°, ë°°ë‚­ ì—¬í–‰ì„ ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

4. **í™ëŒ€**
   - ì Šì€ì´ë“¤ì´ ì¦ê²¨ ì°¾ëŠ” ì§€ì—­ìœ¼ë¡œ, ë‹¤ì–‘í•œ ì¹´í˜, ë ˆìŠ¤í† ë‘, í´ëŸ½ì´ ìˆìŠµë‹ˆë‹¤.

### ì½”ìŠ¤ 3: í˜„ëŒ€ì™€ ì „í†µì˜ ì¡°í™”

1. **ë™ëŒ€ë¬¸ ë””ìì¸ í”Œë¼ì (DDP)**
   - í˜„ëŒ€ì ì¸ ê±´ì¶•ë¬¼ë¡œ, ë‹¤ì–‘í•œ ì „ì‹œì™€ ì´ë²¤íŠ¸ê°€ ì—´ë¦¬ëŠ” ê³³ì…ë‹ˆë‹¤.

2. **ì´íƒœì›**
   - ë‹¤ì–‘í•œ êµ­ì œ ìŒì‹ê³¼ ì¹´í˜ê°€ ìˆëŠ” ì§€ì—­ìœ¼ë¡œ, ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ ê²½í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **ê´‘í™”ë¬¸**
   - ì„œìš¸ì˜ ì¤‘ì‹¬ì— ìœ„ì¹˜í•œ ê´‘ì¥ìœ¼ë¡œ, ë‹¤ì–‘í•œ ê³µì—°ê³¼ í–‰ì‚¬ê°€ ì—´ë¦½ë‹ˆë‹¤.

4. **ì„œìš¸ëœë“œ**
   - ì„œìš¸ ì™¸ê³½ì— ìœ„ì¹˜í•œ í…Œë§ˆíŒŒí¬ë¡œ, ê°€ì¡±ë‹¨ìœ„ ê´€ê´‘ê°ë“¤ì—ê²Œ ì¸ê¸° ìˆëŠ” ê³³ì…ë‹ˆë‹¤.

ì´ ì½”ìŠ¤ë“¤ì€ ì„œìš¸ì˜ ë‹¤ì–‘í•œ ë©´ëª¨ë¥¼ ê²½í—˜í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ì½”ìŠ¤ë§ˆë‹¤ ì‹œê°„ì„ ì¡°ì ˆí•˜ê³ , ê°œì¸ì˜ ê´€ì‹¬ì‚¬ì— ë§ê²Œ ì„ íƒí•˜ì—¬ ë°©ë¬¸í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ ì—¬í–‰ ë˜ì„¸ìš”!
```

### Python code with AutoModel
```python

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

model_id = 'MLP-KTLim/llama-3-Korean-Bllossom-8B'

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

model.eval()

PROMPT = '''You are a helpful AI assistant. Please answer the user's questions kindly. ë‹¹ì‹ ì€ ìœ ëŠ¥í•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.'''
instruction = "ì„œìš¸ì˜ ìœ ëª…í•œ ê´€ê´‘ ì½”ìŠ¤ë¥¼ ë§Œë“¤ì–´ì¤„ë˜?"

messages = [
    {"role": "system", "content": f"{PROMPT}"},
    {"role": "user", "content": f"{instruction}"}
    ]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=2048,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9
)

print(tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True))
# ë¬¼ë¡ ì´ì£ ! ì„œìš¸ì€ ë‹¤ì–‘í•œ ë¬¸í™”ì™€ ì—­ì‚¬, ìì—°ì„ ê²¸ë¹„í•œ ë„ì‹œë¡œ, ë§ì€ ê´€ê´‘ ëª…ì†Œë¥¼ ìë‘í•©ë‹ˆë‹¤. ì—¬ê¸° ì„œìš¸ì˜ ìœ ëª…í•œ ê´€ê´‘ ì½”ìŠ¤ë¥¼ ì†Œê°œí•´ ë“œë¦´ê²Œìš”.

### ì½”ìŠ¤ 1: ì—­ì‚¬ì™€ ë¬¸í™” íƒë°©

1. **ê²½ë³µê¶**
   - ì„œìš¸ì˜ ëŒ€í‘œì ì¸ ê¶ê¶ë¡œ, ì¡°ì„  ì™•ì¡°ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ë¥¼ ì²´í—˜í•  ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤.

2. **ë¶ì´Œ í•œì˜¥ë§ˆì„**
   - ì „í†µ í•œì˜¥ì´ ì˜ ë³´ì¡´ëœ ë§ˆì„ë¡œ, ì¡°ì„ ì‹œëŒ€ì˜ ìƒí™œìƒì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **ì¸ì‚¬ë™**
   - ì „í†µ ë¬¸í™”ì™€ í˜„ëŒ€ ì˜ˆìˆ ì´ ê³µì¡´í•˜ëŠ” ê±°ë¦¬ë¡œ, ë‹¤ì–‘í•œ ê°¤ëŸ¬ë¦¬ì™€ ì „í†µ ìŒì‹ì ì´ ìˆìŠµë‹ˆë‹¤.

4. **ì²­ê³„ì²œ**
   - ì„œìš¸ì˜ ì¤‘ì‹¬ì— ìœ„ì¹˜í•œ ì²œë¬¸ìœ¼ë¡œ, ì¡°ê¹…ê³¼ ì‚°ì±…ì„ ì¦ê¸¸ ìˆ˜ ìˆëŠ” ê³³ì…ë‹ˆë‹¤.

### ì½”ìŠ¤ 2: ìì—°ê³¼ ì‡¼í•‘

1. **ë‚¨ì‚° ì„œìš¸íƒ€ì›Œ**
   - ì„œìš¸ì˜ ì „ê²½ì„ í•œëˆˆì— ë³¼ ìˆ˜ ìˆëŠ” ê³³ìœ¼ë¡œ, íŠ¹íˆ ì €ë… ì‹œê°„ëŒ€ì— ì¼ëª°ì„ ê°ìƒí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.

2. **ëª…ë™**
   - ì‡¼í•‘ê³¼ ìŒì‹ì ì´ ì¦ë¹„í•œ ì§€ì—­ìœ¼ë¡œ, ë‹¤ì–‘í•œ ë¸Œëœë“œì™€ ì „í†µ ìŒì‹ì„ ë§›ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **í•œê°•ê³µì›**
   - ì„œìš¸ì˜ ì£¼ìš” ê³µì› ì¤‘ í•˜ë‚˜ë¡œ, ì¡°ê¹…, ìì „ê±° íƒ€ê¸°, ë°°ë‚­ ì—¬í–‰ì„ ì¦ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

4. **í™ëŒ€**
   - ì Šì€ì´ë“¤ì´ ì¦ê²¨ ì°¾ëŠ” ì§€ì—­ìœ¼ë¡œ, ë‹¤ì–‘í•œ ì¹´í˜, ë ˆìŠ¤í† ë‘, í´ëŸ½ì´ ìˆìŠµë‹ˆë‹¤.

### ì½”ìŠ¤ 3: í˜„ëŒ€ì™€ ì „í†µì˜ ì¡°í™”

1. **ë™ëŒ€ë¬¸ ë””ìì¸ í”Œë¼ì (DDP)**
   - í˜„ëŒ€ì ì¸ ê±´ì¶•ë¬¼ë¡œ, ë‹¤ì–‘í•œ ì „ì‹œì™€ ì´ë²¤íŠ¸ê°€ ì—´ë¦¬ëŠ” ê³³ì…ë‹ˆë‹¤.

2. **ì´íƒœì›**
   - ë‹¤ì–‘í•œ êµ­ì œ ìŒì‹ê³¼ ì¹´í˜ê°€ ìˆëŠ” ì§€ì—­ìœ¼ë¡œ, ë‹¤ì–‘í•œ ë¬¸í™”ë¥¼ ê²½í—˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **ê´‘í™”ë¬¸**
   - ì„œìš¸ì˜ ì¤‘ì‹¬ì— ìœ„ì¹˜í•œ ê´‘ì¥ìœ¼ë¡œ, ë‹¤ì–‘í•œ ê³µì—°ê³¼ í–‰ì‚¬ê°€ ì—´ë¦½ë‹ˆë‹¤.

4. **ì„œìš¸ëœë“œ**
   - ì„œìš¸ ì™¸ê³½ì— ìœ„ì¹˜í•œ í…Œë§ˆíŒŒí¬ë¡œ, ê°€ì¡±ë‹¨ìœ„ ê´€ê´‘ê°ë“¤ì—ê²Œ ì¸ê¸° ìˆëŠ” ê³³ì…ë‹ˆë‹¤.

ì´ ì½”ìŠ¤ë“¤ì€ ì„œìš¸ì˜ ë‹¤ì–‘í•œ ë©´ëª¨ë¥¼ ê²½í—˜í•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê° ì½”ìŠ¤ë§ˆë‹¤ ì‹œê°„ì„ ì¡°ì ˆí•˜ê³ , ê°œì¸ì˜ ê´€ì‹¬ì‚¬ì— ë§ê²Œ ì„ íƒí•˜ì—¬ ë°©ë¬¸í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¦ê±°ìš´ ì—¬í–‰ ë˜ì„¸ìš”!
```



## Citation
**Language Model**
```text
@misc{bllossom,
  author = {ChangSu Choi, Yongbin Jeong, Seoyoon Park, InHo Won, HyeonSeok Lim, SangMin Kim, Yejee Kang, Chanhyuk Yoon, Jaewan Park, Yiseul Lee, HyeJin Lee, Younggyun Hahm, Hansaem Kim, KyungTae Lim},
  title = {Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean},
  year = {2024},
  journal = {LREC-COLING 2024},
  paperLink = {\url{https://arxiv.org/pdf/2403.10882}},
 },
}
```

**Vision-Language Model**
```text
@misc{bllossom-V,
  author = {Dongjae Shin, Hyunseok Lim, Inho Won, Changsu Choi, Minjun Kim, Seungwoo Song, Hangyeol Yoo, Sangmin Kim, Kyungtae Lim},
  title = {X-LLaVA: Optimizing Bilingual Large Vision-Language Alignment},
  year = {2024},
  publisher = {GitHub},
  journal = {NAACL 2024 findings},
  paperLink = {\url{https://arxiv.org/pdf/2403.11399}},
 },
}
```

## Contact
 - ì„ê²½íƒœ(KyungTae Lim), Professor at Seoultech. `ktlim@seoultech.ac.kr`
 - í•¨ì˜ê· (Younggyun Hahm), CEO of Teddysum. `hahmyg@teddysum.ai`
 - ê¹€í•œìƒ˜(Hansaem Kim), Professor at Yonsei. `khss@yonsei.ac.kr`

## Contributor
 - ìµœì°½ìˆ˜(Chansu Choi), choics2623@seoultech.ac.kr
 - ê¹€ìƒë¯¼(Sangmin Kim), sangmin9708@naver.com
 - ì›ì¸í˜¸(Inho Won), wih1226@seoultech.ac.kr
 - ê¹€ë¯¼ì¤€(Minjun Kim), mjkmain@seoultech.ac.kr 
 - ì†¡ìŠ¹ìš°(Seungwoo Song), sswoo@seoultech.ac.kr
 - ì‹ ë™ì¬(Dongjae Shin), dylan1998@seoultech.ac.kr
 - ì„í˜„ì„(Hyeonseok Lim), gustjrantk@seoultech.ac.kr
 - ìœ¡ì •í›ˆ(Jeonghun Yuk), usually670@gmail.com
 - ìœ í•œê²°(Hangyeol Yoo), 21102372@seoultech.ac.kr
 - ì†¡ì„œí˜„(Seohyun Song), alexalex225225@gmail.com